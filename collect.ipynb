{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15ad609a-e1b7-4747-9ab2-11218f34db9a",
   "metadata": {},
   "source": [
    "# Scraping Transfermarkt Data with BeautifulSoup\n",
    "\n",
    "## Setup\n",
    "\n",
    "Before running the script, ensure you have `beautifulsoup4` installed. If not, install it using:\n",
    "```python\n",
    "!pip install beautifulsoup4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f27d5cf-19b9-4a42-a6e5-109243dd2f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c560a293-cb9a-4fd4-9c89-b9392fe1a0d6",
   "metadata": {},
   "source": [
    "## 1. Define Headers and URL\n",
    "To prevent getting blocked, we set a **User-Agent** string that mimics a real browser:\n",
    "```python\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2fb63d7e-4138-4bcc-ac7f-a60fefc0c61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "base_url = 'https://www.transfermarkt.com/spieler-statistik/wertvollstespieler/marktwertetop'\n",
    "params = {\n",
    "    'land_id': '0',\n",
    "    'ausrichtung': 'alle',\n",
    "    'spielerposition_id': 'alle',\n",
    "    'altersklasse': 'alle',\n",
    "    'jahrgang': '0',\n",
    "    'kontinent_id': '0',\n",
    "    'plus': '1'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5207f534-7b22-4e62-9b42-72321523c236",
   "metadata": {},
   "source": [
    "## 2. Fetching Web Pages\n",
    "To request each page, we define a function:\n",
    "```python\n",
    "def fetch_page(page):\n",
    "    params['page'] = page\n",
    "    response = requests.get(base_url, params=params, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        return response.text\n",
    "    else:\n",
    "        print(f\"Failed to fetch page {page}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2738db76-82a8-4448-a56c-243fa3f76481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing page 1...\n",
      "Processing page 2...\n",
      "Processing page 3...\n",
      "Processing page 4...\n",
      "Processing page 5...\n",
      "Processing page 6...\n",
      "Processing page 7...\n",
      "Processing page 8...\n",
      "Processing page 9...\n",
      "Processing page 10...\n",
      "Processing page 11...\n",
      "Processing page 12...\n",
      "Processing page 13...\n",
      "Processing page 14...\n",
      "Processing page 15...\n",
      "Processing page 16...\n",
      "Processing page 17...\n",
      "Processing page 18...\n",
      "Processing page 19...\n",
      "Processing page 20...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "all_players = []\n",
    "\n",
    "def fetch_page(page):\n",
    "    params['page'] = page\n",
    "    response = requests.get(base_url, params=params, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        return response.text\n",
    "    else:\n",
    "        print(f\"Failed to fetch page {page}\")\n",
    "        return None\n",
    "\n",
    "def parse_page(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    table = soup.find('table', class_='items')\n",
    "    if not table:\n",
    "        return []\n",
    "    \n",
    "    rows = table.find_all('tr', class_=['odd', 'even'])\n",
    "    players = []\n",
    "    \n",
    "    for row in rows:\n",
    "        cols = row.find_all('td')\n",
    "        if len(cols) < 15:\n",
    "            continue\n",
    "        \n",
    "        player_td = cols[1]\n",
    "        name = cols[3].get_text(strip=True)\n",
    "        position = cols[4].get_text(strip=True)\n",
    "        age = cols[5].get_text(strip=True)\n",
    "        \n",
    "        primary_nat = cols[6].find(\"img\")[\"alt\"] if cols[6].find(\"img\") else cols[6].get_text(strip=True)\n",
    "        secondary_nat = cols[6].find_all(\"img\")[-1][\"alt\"] if len(cols[6].find_all(\"img\")) > 1 else \"-\"\n",
    "        \n",
    "        club = cols[7].find(\"img\")[\"alt\"] if cols[7].find(\"img\") else cols[7].get_text(strip=True)\n",
    "        \n",
    "        market_value = cols[8].get_text(strip=True)\n",
    "        \n",
    "        matches = cols[9].get_text(strip=True)\n",
    "        goals = cols[10].get_text(strip=True)\n",
    "        own_goals = cols[11].text.strip() if cols[11].text else ''\n",
    "        assists = cols[12].get_text(strip=True)\n",
    "        yellow_cards = cols[13].get_text(strip=True)\n",
    "        second_yellow = cols[14].text.strip() if cols[14].text else ''\n",
    "        red_cards = cols[15].get_text(strip=True) if len(cols) > 15 else \"\"\n",
    "        sub_in = cols[16].get_text(strip=True) if len(cols) > 16 else \"\"\n",
    "        sub_out = cols[14].text.strip() if cols[14].text else ''\n",
    "        \n",
    "        player_data = {\n",
    "            'Name': name,\n",
    "            'Position': position,\n",
    "            'Club': club,\n",
    "            'Market Value': market_value,\n",
    "            'Age': age,\n",
    "            'Primary Nationality': primary_nat,\n",
    "            'Secondary Nationality': secondary_nat,\n",
    "            'Matches Played': matches,\n",
    "            'Goals': goals,\n",
    "            'Assists': assists,\n",
    "            'Yellow Cards': yellow_cards,\n",
    "            'Second Yellow Cards': second_yellow,\n",
    "            'Red Cards': red_cards,\n",
    "            'Substituted In': sub_in,\n",
    "            'Substituted Out': sub_out,\n",
    "            'Own Goals': own_goals\n",
    "        }\n",
    "        players.append(player_data)\n",
    "    \n",
    "    return players\n",
    "\n",
    "for page in range(1, 21):  \n",
    "    print(f\"Processing page {page}...\")\n",
    "    html = fetch_page(page)\n",
    "    if html:\n",
    "        players = parse_page(html)\n",
    "        all_players.extend(players)\n",
    "    sleep(1) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b94ac11-b054-4362-bfc8-8901eb4e47a0",
   "metadata": {},
   "source": [
    "## 3. Saving Scraped Data to CSV with Pandas\n",
    "\n",
    "After collecting player data, we store it in a **Pandas DataFrame** for structured handling:\n",
    "```python\n",
    "df = pd.DataFrame(all_players)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25393f6c-1af2-4bc9-a732-3412054ed942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data collection complete. Saved to collected.csv\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(all_players)\n",
    "df.to_csv('collected.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "print(\"Data collection complete. Saved to collected.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180cfba9-fc3c-45c0-9eed-d627fd266a9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
